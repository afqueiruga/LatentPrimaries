---

title: Solution of Differential Algebraic Equations with Manifold Constraints on the Latent Space of an Autoencoder
author:  Alejandro Francisco Queiruga
date: 2018-9-20
abstract: We train an autoencoder to determine the lower-dimensional representation of the EOS surface using modern deep learning techniques without using any phase labels. The decoder end of the model is then plugged back into the balance laws, eliminating the constrain equation.
The simulation unknowns are now variables on the latent space of the autoencoder (the inputs of the decoder), which can be solved in a straight forward manner with Newton's method and no phase-switching logic. This provides a more robust numerical algorithm, while putting the onus of programming part of the simulation itself onto the computer: tens of thousands of lines of human written code consisting of empirical fits and branching logic are replaced by code generated by the learned model.
---

---

# Introduction

Representing complex physical systems with multiple speicies, phases, chemical reactions, and heterogenous scale effects poses an intractability problem to modeling.  How do we analyze complex experimental data and *ab initio* simulations and derive new macroscale relations to describe the systems? How do we then implement these relations in the rest of our analyses and production-scale computer simulations? A single material with multiple phase transitions is a challenge in of itself, and the descriptions become combinatorially more complex as species and phases are added. This results in combinatorially more human labor and combinatorially more code which tends to involve complex branching logic. The descriptions then pose numerical challenges when incorporated into macroscale simulations due to obstacles such as discrete phase state machines and primary variable switching.

Motivations for this work are subsurface energy systems such as shale and oceanic natural gas reservoirs in shales or engineered geothermal systems.

This work is only considered with the "single beaker" problem; the flow and transport aspects have similar challenges. (The next stage of this work will "connect the beakers".)

This work proposes a new methodology that steps away from the traditional approaches 

In this approach, we directly *look for solvable representations* instead of representations that carry human intuition. We value simulation runtime and robustness. Further, we must remember the value of the human effort involved: beyond improving computational aspects, we are fully automating part of the human involvement in programming the simulator.

Part of the new modern software engineering framework involves replacing human-written code with computer-written code. We are applying this mindset from the computer-vision and advertisement dominated Artificial Intelligence space into scientific computing.

The actual meaning of the parameterization is not important, so long as it can be effectively solved and then *decoded* back to the physically meaningful quantities.

The original  redescribe our problem as a differential algebraic equation with a constraint: the balances of mass and energy are constrained by the EOS. Because the EOS was originally derived from data, a data-driven approach lends itself in which the EOS is a set of points in temperature-pressure-density-enthalpy space. (In the multicomponent cases there are more dimensions.) 

We demonstrate this approach to solving constrained differential equations on the pendulum, before demonstrating two EOSes of water: one with just liquid-gas phases, and one with liquid-gas-ice-supercritical regimes. The notion of phases does not appear in this multiphase simulation. We show a model architecture that is able to learn phase labels in the unsupervised autoencoder training process, such human-scientist interpretable results are obtained from the machine learning process. We compare this and other model architectures, including common deep neural networks, based on autoencoder error and simulation success metrics. We stress that the presented approach is only utilizing machine learning on material relations derived from empirical data: the simulation is indeed solving physical balance laws.

Our proposed approach is to build upon modern machine learning techniques to aid in equation discovery. An autoencoder is trained on the original material dataset to learn a lower-dimensional representation of the underlying manifold. The model is designed to be directly incorporated into a macroscale simulation of physical balance laws, where now the traditional partial differential equations are being solved on the learned latent space. This provides a more robust numerical algorithm, while putting the onus of programming part of the simulation itself onto the computer: tens of thousands of lines of human written code consisting of empirical fits and branching logic are replaced by code generated by the learned model. The methodology will first be demonstrated on datasets based on thermodynamic intensive variables, which are already challenging to analyze, before being extended to richer datasets such as experimental images or molecular dynamics. We stress that the proposed approach is only utilizing machine learning on material data: the simulation is indeed solving physical balance laws. Through deep learning approaches combined with domain-specific goals, we can learn feature sets that are both interpretable and directly apply to analyzing larger systems. 

Consider a general differential algebraic equation of the form

$$
\begin{equation}
z(\dot{u},u) = 0
\end{equation}
$$

We are interested in equations that have the form
$$
\begin{align}
\frac{\mathrm{d}u}{\mathrm{d}t} & = v \\
0 & = c(u,v)
\end{align}
$$
Where do such equations arise? A simple example is the pendulum, which will be explored in Section 2.
$$
\begin{align}
\text{Solve for}\, x(t), \, y(t), \, f(t) \, \text{satisfying:} \\
m \dot{x} & = f x/L \\
m \dot{y} & = f y/L + m g \\
x^2 + y^2 & = L^2
\end{align}
$$



# Multiphase Equations of State

We also consider the phase equilibria; at a larger scale it is possible for a control volume to be a mixture of two phase coexisting, such as boiling with some mixture saturation $S_{gas}=1-S_{liq}$.

For this preliminary work, we consider only a single-beaker problem without flow. Let $\rho$ be the material density, $T$ be the temperature, $p$ be the material pressure, $h$ be the material enthalpy, and $u$ be the internal energy. continuous stirred tank reactor There are two equations for balance of mass and energy. 
$$
\begin{align}
\partial_t \rho & = \mathbf{k}_p(p_\infty - p) + r\\
\partial_t \rho u & = \mathbf{k}_T(T_\infty-T) + s
\end{align}
$$
The beaker is exposed to a reservoir with $p_\infty$ and $T_\infty$ via a mixed boundary condition straw  with transport and conduction coefficients $\mathbf{k}_p$, $\mathbf{k}_T$. Mass and heating sources and sinks are the terms $r$ and $s$.[^enthalpy]

[^enthalpy]: We express the material by enthalpy instead of internal energy; they are related by the relation $\rho u = \rho h - p$. Future work may work directly with internal energy.

The typical methodology is to use the empirical relations for density and enthalpy $h$ as a function of pressure and temperature and solve the DAE for $p$ and $T$ implicitly: 
$$
\begin{align}
\text{Solve for}\, p(t)\, \text{and}\, T(t)\, \text{such that:}\\
\partial_t \rho(p,T) & = \nabla \cdot \mathbf{k}\nabla p + r\\
\partial_t \rho u(p,T) & = \nabla \cdot \mathbf{k'}\nabla T + s
\end{align}
$$
The complicatation is that the functions $\rho(p,T)$ and $u(p,T)$ are not well defined functions due to the presence of phase changes that yield sharp discontinuities in $p,T$, as shown in Figure 1 for water. The material can exist on this section of the surface as a mixture of the two phases.


$$
\rho = X_{H_2O}\left(S_{gas} \rho_{gas} + S_{liq} \rho_{liq} + S_{ice} \rho_{ice} + S_{crit} \rho_{crit} \right)
$$

We will also note right now that applicaitons also involve continuum mechanics and chemical reactions! The interplay of deformations, reactions, and phase transitions makes the equations extremely difficult to solve.

One possible definition of phase boundaries:

> Phase transitions occur when the thermodynamic free energy of a system is non-analytic for some choice of thermodynamic variables (cf. phases).
>
> https://en.wikipedia.org/wiki/Phase_transition



![phase boundaries of $\H_2O$](/Users/afq/Documents/Dropbox/ML/primaryautoencoder/figures/phase_diagram.png)

![equation of state surface of $\H_2O$](/Users/afq/Documents/Dropbox/ML/primaryautoencoder/figures/water_eos.png)

## State Machine logic

Formulate balance equations as

$$
R(X ; phase) = 0
$$

Solve the differentiable part:
$$
\frac{\partial R}{\partial X}\Delta X^{k+1/2} = -R(X^k ; phase^k)
$$
and iterate on the nondifferential part:
$$
phase^{k+1},X^{k+1} = statemachine(X^{k+1/2} ; phase^k)
$$

The phase switching logic has the form of a switch case:

```C
switch phase_old:
  case gas:
    if p,T crossing boundary:
      phase_new = liquid_gas
  case liquid:
    if p,T crossing boundary:
      phase_new = liquid_gas
  case liquid_gas:
    if S_gas >= 1:
      phase_new = gas
    if S_liquid >= 1:
      phase_new = gas
```

## A constrained Differential Algebraic Equation

We argue that the relations that are used are not the *ground truth* in of themselves. Beyond ideal gases and simple linear fluids, the functions $\rho(p,T)$ or $p(\rho,T)$ are simply complicated fits that were derived in an ad-hoc piecewise fashion from experimental data. This is not a bad thingâ€”it is the fundamental nature of the problem. When we look at the corpus of literature on a material as water, what we actually have is a decision branch to multiple different complicated fits for each material branch. 

We instead replace our problem with two balance laws and one unknown constraint:
$$
\begin{align}
\text{Solve for}\, \rho(t), \, h(t), \, p(t),\, \text{and}\, T(t)\, \text{satisfying:}\\
\partial_t \rho & = \nabla \cdot \mathbf{k}\nabla p + r\\
\partial_t \rho h - p & = \nabla \cdot \mathbf{k'}\nabla T + s\\
\text{such that they lie on the material EOS,}\\
EOS(\rho,h,p,T) & = 0
\end{align}
$$
The *ground truth* is the experimental data in the first place; the branching curve fit is only one realization of representing the data. We have demonstrated that it is not possible to represent any two of these in terms of the other two; and even if so, it is not necessarily a good and robust fit. What if we search for a new fit?

# The Pendulum

An archetypical constraint problem the pendulum. Posed as dynamics, it has three unknowns wtih two second-order equations and a constraint,
$$
\begin{align}
\text{Solve for}\, x(t), \, y(t), \, f(t) \, \text{satisfying:} \\
m \ddot{x} & = f x/L \\
m \ddot{y} & = f y/L + m g \\
x^2 + y^2 & = L^2
\end{align}
$$
In our methodology, we do not have an explicit equation for the constraint, but a dataset of (x,y) pairs that are on the manifold. For our proof-of-concept test, we manufacture this data for the pendulum. Note that we are considering the nonlinear pendulum.

There are many ways to solve this cannonical equation: Lagrange multipliers ($f(t)$ is the multiplier in the way we have written it now), penalties, change of variables, etc. The change of variables method is to introduce a new variable $\theta$ that parameterizes $x=L\cos\theta$ and $y=L\sin\theta$:
$$
\begin{align}
\text{Solve for}\, \theta(t) \text{and} \, f(t) \, \text{satisfying:} \\
m \ddot{x}(\theta) & = f x(\theta)/L \\
m \ddot{y}(\theta) & = f y(\theta)/L + m g \\
\end{align}
$$
In this example, this lets us collapse the problem into a single-component Ordinary Differential Equation. 

The manifold for this simple problem is smooth, so two nested polynomials forms the autoencoder:
$$
\left\{\begin{array}{c} x\\y\end{array}\right\}
\rightarrow W^1 x^n \rightarrow q \rightarrow W^2 x^n \rightarrow 
\left\{\begin{array}{c} x\\y\end{array}\right\}
$$

(We explicitly write the bias $+b$; it is equivalent to including $x^0=1$ in the polynomial set.) The Lagrangian for the pendulum is 
$$
\begin{equation}
L = \frac{1}{2}m\left(\dot{x}^2 + \dot{y}^2\right) - m g y
\end{equation}
$$
with the constraint that $x$ and $y$ lie on the path. The equations of motion given some parameter space that satisfies the constraint is
$$
\begin{equation}
\frac{\mathrm{d}q}{\mathrm{d}t}\frac{\partial L}{\partial \dot{q}} = \frac{\partial L}{\partial q}.
\end{equation}
$$
We'll use the autoencoder to generate the parameterization of $x(q)$ and $y(q)$. By the chain rule, the velocities are related to the feature space by 
$$
\begin{equation}
\dot{x} = \frac{\mathrm{d}x(q)}{\mathrm{d}t} = \frac{\partial x}{\partial q}\frac{\mathrm{d}q}{\mathrm{d}t}
\end{equation}
$$
and similarly for the $y$ direction.

Taking the partial derivatives with respect to the feature variable and its rate, the components to the equation of motion are 
$$
\begin{equation}
\frac{\partial L}{\partial q} = m g \frac{\partial y}{\partial q}
\end{equation}
$$


When we want to solve the dynamics, we can treat the autoencoder the same as $x(\theta)$, plugging in 
$$
L(x(q),v(q,\dot{q}))
$$
and build the DAE we know from physics:
$$
\frac{\mathrm{d}}{\mathrm{d}t}\frac{\partial L}{\partial \dot{q}} = \frac{\partial L}{\partial q}
$$
where we get the velocity with the chain rule:
$$
v = \frac{\partial x}{\partial q} \dot{q}
$$

Then plug it into a Runge Kutta (trapezoidal) to solve with Newton's method:
$$
\frac{\partial L}{\partial \dot{q}}_i - \Delta t \frac{\partial L}{\partial q}_i = \frac{\partial L}{\partial \dot{q}}_0 + \Delta t \frac{\partial L}{\partial q}_0
$$

![results of the pendulum](/Users/afq/Documents/Dropbox/ML/primaryautoencoder/figures/pendulum_q.png)

# Representation Learning for Reparameterization

The central idea of this work is to obtain two new variables solve for those directly,
$$
\begin{align}
\text{Solve for}\, u(t)\, \text{and}\, v(t)\, \text{such that:}\\
\partial_t \rho(u,v) & = \nabla \cdot \mathbf{k} \nabla p(u,v) + r \\
\partial_t \rho e(u,v) & = \nabla \cdot \mathbf{k'}\nabla T(u,v) + s
\end{align}
$$
where $u$ and $v$ have no physical meaning other than simply being a parameterization of the equation of state constraint. We want to select $u$ and $v$ to require no auxilary phase index to define the system and no additional logic in the code. The sharp kinks and possible discontinuities can be a part of the functions $\rho(u,v), p(u,v), e(u,v)$ and $T(u,v)$.

Can we automatically discover this parameterization?
(For a such a simple equation, we do not think the proposed methodology will be better, but the pendulum will instead be our *unit test*.)
Our proposed method is motivated by the problems of multiphase reaction and transport, where the underlying constraints are the ill-behaved material properties which are difficult to describe. We change our perspective on these problems in the next section.

## Latent Space

Instead of the painstaking work of parameterizing phase boundaries and determining curve fits by hand, we rephrase the problem of representing the equation of state by learning and autoencoder. We have an encoding phase, $E(\rho,p,e,T; a)$ and a decoding phase, $D(u,v; b)$ that forms an identity function with a compressed subspace:
$$
\begin{equation}
\left\{ \rho, p, e, T \right\} \rightarrow  E \rightarrow \left\{ u,v \right\} \rightarrow D \rightarrow \left\{ \rho, p, e, T \right\}
\end{equation}
$$
The autoencoder is solved for by optimizing its parameters using the goal
$$
\begin{equation}
\min_a \sum_x \left( x - D(E(x;a);a) \right)^2
\end{equation}
$$
Note that we are not yet considering constraints $c$ with partial differential equation components in space. These types of material equations of states are only enforced pointwise. The time components of the equations can contain spatial derivatives; i.e. this method fits easily inside a finite volume simulation with little change.

## Training

We use a Euclidean norm loss function with a contractive penalty: 
$$
L(x)=\left\|x-D(E(x))\right\|^2_2+\lambda\left\|\frac{\partial E}{\partial x}\right\|_F^2
$$
The gradient on the encoder is a $4\times2$ matrix in our case, and $\|\|_F$ denotes the Frobenius norm. The contractive penalty $\lambda$ smooths out and evenly distributes the mapping to $q$.

The variational autoencoder is a little more complicated to implement, and we do not necessarily want to match a Gaussian interior. Because we generated our datasets artificially (both in this manufactured setting and in an actual laboratory setting), the dataset does not reflect draws from a distribution, so the KL divergence against a gaussian is not quite applicable. 

We note that penalizing the decoder would smooth out the sharp kinks at phase boundaries.

We train in two phases. The first step is minibatched stochastic gradient descent on the entire deep network. After a few epochs of this, we do a step of linear regression on the last layer of the network, denote by $W^{dec}$, which directly controls the values. The linear system that needs to be solved is easily generated by assembling the Hessian of the total loss function,
$$
\mathbf{G} = \sum_{x\in X}\frac{\partial L}{\partial W^{dec}}\\
\mathbf{H} = \sum_{x\in X}\frac{\partial^2 L}{\partial (W^{dec})^2}
$$
and solving the linear system for an update to the last parameters.
$$
[\mathbf{H}] \Delta W^{dec} = -\{\mathbf{G}\}
$$
 To form the hessian and gradient for this step, we sum over all of the training data. (The bias $b^{dec}$ is included in $W^{dec}$.) (The contractive penalty term drops out because it does not depend on $W^{dec}$.)

In future work, we would want to continue appending new $q$s in a systemic way to derive, e.g., an $H_2O + NaCl$ equation of state with 3 degrees of freedom, bootstraping from the pre-trained $H_2O$ architecture with 2 degrees of freedom.

## Encoder Initializations

The properties $p,T$ are the usual prefered choice for primary variables, but as discussed above, the equations of state are not analytic for any choice. (Indeed, this idea first spawned when the author was thinking about systematically rewriting equations in $\rho,h$.) Ideally, this system would be fully automated, but due to the nonlinearilty of the autoencoder problem there are many "bad" options the trainer can fall into. Multiple starts are possibly needed to get a good solution when starting from purely randomized newtork parameters. Global optimization is truly a necessity when trying to optimize networks with so few parameters. We incorporate a little bit of human intuition and preference by trying three options to start off the autoencoder: 1) fully random, 2) $p,T$ prefered, 3) $\rho,h$ prefered. This is implemented by added a bias to the initialization of the encoder coefficients, e.g. for $p,T$

$$ W^{init}_{enc} = \left[\begin{array}{cccc}
1 & 0 & 0 & 0 & â€¦ \\
0 & 1 & 0 & 0 & ...
\end{array}\right]+[\sigma]$$

where $[\sigma]$ is the random initialization and $â€¦$ denotes padding the  coefficients to the nonlinear terms by 0s. Recall that the values for $s$ have already been rescaled to be around 0 with a range of 1. The random initialization part is decided by $\sigma \sim \mathcal{N}(0,0.1)$ .



## Unsupervised Classification



![classifying architecture](/Users/afq/Documents/Dropbox/ML/primaryautoencoder/slides/classifier_network.png)

## 

# Simulation

Solve for $q_1(t)$ and $q_2(t)$ such that:
$$
\begin{align}
\partial_t \rho(q_1,q_2) & = \nabla \cdot \mathbf{k} \nabla p(q_1,q_2) + r \\
\partial_t \rho h(q_1,q_2)-p & = \nabla \cdot \mathbf{k'}\nabla T(q_1,q_2) + s
\end{align}
$$
where $\rho(q_1,q_2)$ etc. are the back of an autoencoder:
$$
\begin{equation}
\left\{ \begin{array}{c}
T\\ p\\ \rho\\ h
\end{array}\right\} \rightarrow  E \rightarrow 
\left\{ \begin{array}{c} q_1\\q_2 \end{array} \right\}\rightarrow D \rightarrow 
\left\{ \begin{array}{c}
T\\ p\\ \rho\\ h
\end{array}\right\}
\end{equation}
$$
Using the autoencoder to automatically remove the local constraint:
$$
\begin{equation}
eos(D(q_1,q_2)) = 0 \, \forall \,q_1,q_2
\end{equation}
$$
**Only training the material representation, not the balance laws.**

First, we had a constrained DAE:
$$
\begin{align}
\frac{\mathrm{d}}{\mathrm{d}t} m(T,p,\rho,h) &= r(T,p,\rho,h)\\
eos(T,p,\rho,h) &= 0
\end{align}
$$
with no good representation of $eos$ everywhere.

The autoencoder parameterizes the constraint :
$$
\begin{equation}
\frac{\mathrm{d}}{\mathrm{d}t} m(D(q)) = r(D(q))
\end{equation}
$$
We can parameterize this equation with any time stepping scheme, .e.g. a Runge-Kutta scheme. For subsurface flow, backward Euler is popular due to its simplicity and L-stability, which we follow since the topic of this paper is not exploring new time stepping schemes. This yields the discretizated algebraic equation for $q_i$ at $t+\Delta t$ given the value $q^0$ at $t$,
$$
\begin{equation}
0 = R(q^i) = m(q^{i}) - m(q^{0}) - \Delta t \, r(q^i)
\end{equation}
$$
where we have called the quation $R(q)$. By design $R(q)$ is smooth enough to solve with Newton's method without any statemachine logic:
$$
\begin{equation}
\frac{\partial R}{\partial q} \Delta q^{k+1} = R(q^k)
\end{equation}
$$
The function $R(q)$ has complex logic encoded in it; if-then statements appear if the model used activations such as rectifiers.

## Initialization

There is a caveat with representing the degrees of freedom of the system on a learned space: the user does not know the meaning of $q$ as an input. Further, the values of $q$ vary from network to network and depend on the parameterization. The user needs to be able to specify the "real" physical quantities.

The user specifies any two of the values, 

Note that the encoder only necessarily works when all four values are correct. Thus, it is necessary to actually solve the nonlinear problem on the decoder:

$$
D(q^0)_i = s^0_i \quad \text{for specified}\; i
$$

This equation has two rows. It is solved with Newton's method until convergence. 


$$
K_ij = \frac{\partial D_i}{\partial q_j}  \quad \text{for specified}\; i, \text{for}\;j=1,2
$$

$$
K\Delta q = s^0_i-D(q)_i
$$

The problem is not gaurunteed to converge, however, and often does not. E.g., if the initial guess was in the liquid phase and the target point in the EOS space is in a solid, the iteration will fail. To address this issue, we perform multiple starts with an initial condition in each phase and equilibrium, discard points that diverge, and then pick the $q$ that is closest to the two specified components.

It is possible that none of the initial guesses for this iteration converge. There are two possiblities: the target state point is incorrectly specified or out-of-range (e.g., negative density), or the network is incapable of representing the desired material properties (i.e., poorly trained).

The weakness of this necessity means that more error checking is needed in verifying user inputs, but this is necessary in any multiphase/multicomponent reservoir simulator. LatentSim does not yet detect and report errors, and only reports divergence. In our study, the testing problem specifications are all correct, but not all network realizations are acceptable. Failure to determine a $q$ for one of the test initial conditions is a common mode of detecting unacceptable network realizations; this is marked as a testing error during training which invalidates a model as a potential realization.



The current implementation LatentSim does not support inputting via saturations on the equilibria, a commonly used primary variable, but this problem could be solved with future engineering work for a non-experimental simulator.

## Methodology Summary

1. Make a database of $T,p,\rho,h,X_1,...$  
   - Piece together empirical fits for each phase from literature
   - (*Experimental data in the future*)
2. Normalize (and $\log(p)$) the database, shuffle it  
   - $\log(p)$ distributes the low-$p$ phases evenly w.r.t. high-$p$ phases
3. Train the autoencoder on batch-generated architectures. Note that phase labels are not used.
4. Load the models and generate physics code
5. Verify and grade architectures on tests
   - Need more than autoencoder mean-squared-error
   - Differentiability and numerical stability in simulation
   - Evaluation speed (billions of times in a simulation!)
6. Pick best one to package into production code



## Pseudo-code

```
load network from database
build DAE graph
input problem properties and initial value
solve for q0: D(q0) = s0
loop t=0 to t_max:
	solve for q[i]: R(q[i],q[i-1])=0
decode s[i] = D(q[i])
```



# Implementation

The training framework and simulations are implemented in TensorFlow using custom defined operations.
The various visualizations are built using Matplotlib, Plotly, and Visdom.
The accompanying video was generated using Paraview.

The codebase is released open source at 
[https://github.com/afqueiruga/latentsim](https://github.com/afqueiruga/latentsim).
Helper methods that the author reuses in various TensorFlow-based projects are factored out in the dully-named [https://github.com/afqueiruga/afqstensorflowutils](https://github.com/afqueiruga/afqstensorflowutils).
The implementation of the empirical equations of state for water are factored out at [https://github.com/afqueiruga/equations_of_state](https://github.com/afqueiruga/equations_of_state).

# Simulation Evaluation

Multiple hyperparameters are trained.

Each architecture trained on the autoencoder task is then 

Three different extents:
1. Linear Equation of State  
  -  **Test!**
  - Reduces to single phase Darcy's law problem (slide 1)
  - $ p = 10^5+[-10^3, 10^3] Pa,\quad T = [ 19, 21 ] ^o C$
2. Water Liquid-Gas Regime  
  - One phase boundary  
  - $ p = [100,5\times 10^5] Pa, \quad T = [274,594] K$
3. Water Solid-Liquid-Gas-Supercritical Regimes  
  - No linear mapping to latent space  
  - Entire span
  - $ p = [6\times 10^{-6},3\times 10^8]Pa, \quad T = [150,1000] K $

## Benchmarks

The simulator was benchmarke against the long standing TOUGH+ multiphase multicomponent

Single-grid block simulations were set up for most of the cases. TOUGH+ does not handle supercritical $H_2O$, so these test problems were not used to benchmark.  (The supercritical phase region is not practically relevant for water, but it is very important for other fluids such as $CO_2$.) (Note that, as a major motivation of this work, adding supercritical states to the existing Fortran simulator would require coding of one new logical state with two new state transitions with primary variable switches; for the approach of this paper, the dataset just needed to be extended.)

# Conclusions

- Deep learning to replace and improve hand-baked equations and algorithms
- 
- **Only training the material representation, not the balance laws**
- Extend to more complicated materials
- Put into a flow simulation
- Close loop on testing with reinforcement learning

The ultimate goal for this work is to automate integration of data analysis into simulation.

We hypothesize this end-to-end approach will allow the system to develop *better* simulations *faster*. 

We demonstrated a specially crafted architecture whose parameters can be human interpretted and perform unsupervised phase/equilibria classification.

We illustrated a programming model that is able to incorporate machine learning models into existing well known computational methodologies. This allows us to enforce physically known quanities, such as the balance laws.

The TensorFlow system is not quite suited for this type of workflow -- the resulting simualtions are extremely slow due to repeated calls to session.run and a very complicated graph structure for the polynomials.
Currently, the system is being reimplemented in Julia using Flux and Zygote to seemlessly reincorporate into a fully-fledged new reservoir simulator.
The pendulum has been reimplemented in Julia at .



We are also demonstrating a new programming paradigm applied to scientific computing. The entire system has another level of complexity to it. An offline training phase searches for a new representation, i.e. for new equations with new unknown degrees of freedom, that are good to solve. 

Each implemented equation of state (three options in this work spanning different extents of $H_2O$) has a database of potential realizations. The entries are model architectures, parameters, and training environment configurations and logs for each equation-of-state dataset. This database of models replaces a version controlled repository of user written modules. Hundreds of trials for novel primary variable configurations happen in a matter of minutes on a workstation in place of pain-staking trial-and-error labor performed with human "intuition" and experience. We did encode a little bit of human intuition into the model architectures and tricks for initializing the networks.

The larger simulation code links into the trained database to load the computer-written model into the computation graph for the simulation, deriving all needed ingredients for the mainloop using automatic differentiation.

A fully automated testing system can verify which realizations worked (relate this to an independent review board comparing the results the independently developed codes) and then select the fastest and most robust one.

